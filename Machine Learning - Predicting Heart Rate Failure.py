# -*- coding: utf-8 -*-
"""Project_COMP_487.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eCBYPtW7bgrBECQchbVCYH4totjdIXur

Hi everyone, we will use this Colab Notebook to combine our code

Dataset: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction
"""

###### COMP 479 - Project ######
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

#Read in datafile
df= pd.read_csv("/content/heart.csv")
df2 = pd.get_dummies(df, columns = ['Sex', 'ChestPainType', 'RestingECG', 'ST_Slope','ExerciseAngina'])


##add code to show there are no Null values
#test train splitting
X = df2.iloc[:, :-1]
y = df2.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=55, test_size=0.3, shuffle=True)

print(X.head())

from google.colab import drive
drive.mount('/content/drive')

"""#ADALINE"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Splits the predictors and the response
X = df2.drop(['HeartDisease'], axis=1)
y = df2['HeartDisease']

# Splits training and testing data sets
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=13, shuffle=True)

# Separates numerical and categorical columns
numeric_columns = X.select_dtypes(include=['float64','int64']).columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Standardizes Features
scaler = StandardScaler()
X_train_std = X_train[numeric_columns]
X_test_std = X_test[numeric_columns]

X_train_std.loc[:, numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
X_test_std.loc[:, numeric_columns] = scaler.transform(X_test[numeric_columns])


## ADAptive LInear NEuron ##

# Adaline model with Stochastic Gradient Descent (SGD) Classifier
adaline_ml = SGDClassifier(loss='perceptron', learning_rate='constant', eta0=0.01, random_state=13)
adaline_ml.fit(X_train_std,y_train)

# Prediciton on Train Set #

# Predicts the response on train set
y_hat_tr = adaline_ml.predict(X_train_std)
y_hat = adaline_ml.predict(X_test_std)

# Evaluates the model on train set
confusion_mtx_tr = confusion_matrix(y_train, y_hat_tr)
class_report_tr = classification_report(y_train, y_hat_tr)

# Shows the results
print("\nPredicting on Train Set:")
print("\nConfusion Matrix: \n", confusion_mtx_tr)
print("\nClassification Report: \n", class_report_tr)

# Prediciton on Test Set #

# Predicts the response on test set
y_hat = adaline_ml.predict(X_test_std)

# Evaluates the model on test set
confusion_mtx = confusion_matrix(y_test, y_hat)
class_report = classification_report(y_test, y_hat)

# Shows the results
print("\nPredicting on Test Set:")
print("\nConfusion Matrix: \n", confusion_mtx)
print("\nClassification Report: \n", class_report)

"""#SVM"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler

X = df2.drop(['HeartDisease'], axis=1)
y = df2['HeartDisease']

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=55, shuffle=True)

numeric_columns = X.select_dtypes(include=['float64','int64']).columns
categorical_columns = X.select_dtypes(include=['object']).columns
scaler = StandardScaler()
X_train = X_train[numeric_columns]
X_test = X_test[numeric_columns]

X_train.loc[:, numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
X_test.loc[:, numeric_columns] = scaler.transform(X_test[numeric_columns])

param_grid_linear = {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
    'penalty': ['l1', 'l2']}

linear_svm = LinearSVC(random_state=55, dual=False)
grid_search_linear = GridSearchCV(estimator=linear_svm, param_grid=param_grid_linear, scoring='accuracy', cv=5)

grid_search_linear.fit(X_train, y_train)
best_params_linear = grid_search_linear.best_params_
best_score_linear = grid_search_linear.best_score_
best_linear_model = grid_search_linear.best_estimator_
y_pred_linear = best_linear_model.predict(X_test)

print("Best Parameters for LinearSVC:", best_params_linear)
print("Best Score for LinearSVC:", best_score_linear)
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred_linear))
print("Classification Report: \n", classification_report(y_test, y_pred_linear))

"""# Logistic Regression

"""

df2 = pd.get_dummies(df, columns = ['Sex', 'ChestPainType', 'RestingECG', 'ST_Slope','ExerciseAngina'],drop_first=True)

X = df2.drop(['HeartDisease'], axis=1)
y = df2['HeartDisease']
X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=55, test_size=0.2, shuffle=True)
print(X_train.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'penalty': ['l1', 'l2'],
    'solver': ['saga']
}

logreg = LogisticRegression()
grid_search = GridSearchCV(logreg, param_grid, scoring='accuracy', cv=5)
grid_search.fit(X_train, y_train)
print("Best Parameters: ", grid_search.best_params_)

from sklearn.metrics import accuracy_score

logistic = LogisticRegression(C=0.1, penalty='l2')
logistic.fit(X_train, y_train)
y_pred_log = logistic.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_log)
print("Accuracy on Test Set: {:.2f}%".format(accuracy * 100))
print()
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred_log))
print()
print("Classification Report: \n", classification_report(y_test, y_pred_log))

feature_names = X.columns.tolist()
print(feature_names)

coefficients = logistic.coef_.flatten()
feature_coefficients = list(zip(feature_names, coefficients))

# Print feature names and corresponding coefficients
for feature, coefficient in feature_coefficients:
    print("{}: {:.4f}".format(feature, coefficient))

# If you want to print the intercept as well
print("Intercept: {:.4f}".format(logistic.intercept_[0]))

quant = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']

categorical = [x for x in feature_names if x not in quant]
print(categorical)

for feature, coefficient in feature_coefficients:
  if feature in categorical:
    if coefficient < 0:
      print(f'{feature} decreases the odds of heart attack by {(1-np.exp(coefficient))*100}% compared to baseline')
    else:
      print(f"{feature} increases the odds of heart attack by {(np.exp(coefficient)-1)*100}% compared to baseline")
  else:
    if coefficient < 0:
      print(f'A one-unit increase in {feature} decreases the odds of heart attack by {(1-np.exp(coefficient))*100}%')
    else:
      print(f"A one unit increase in {feature} increases the odds of heart attack by {(np.exp(coefficient)-1)*100}%")

"""# Neural Network

"""

import numpy as np
import warnings
from numpy.random import randn
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')

X = df2.drop(['HeartDisease'], axis=1)
y = df2['HeartDisease']
X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=55, test_size=0.3, shuffle=True)

samples = 642
trait_num = 20
output = 1

x = X_train
y = y_train.values.reshape(-1, 1)

iterations = 500
weight_iter = 14

loss_vals = np.zeros(iterations,dtype=float)
i_vals = np.zeros(iterations,dtype=float)
training_accuracy_vals = np.zeros(weight_iter,dtype=float)
testing_accuracy_vals = np.zeros(weight_iter,dtype=float)
weight_num_vals = np.zeros(weight_iter,dtype=float)

for weight_num in range(weight_iter):
  print("\nNumber of weights: " + str(weight_num+1))
  weight_num_vals[weight_num] = weight_num +1
  w1, w2 = randn(trait_num, weight_num+1), randn(weight_num+1, output)
  for i in range(iterations):
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)

    loss = np.sum(np.square(y_pred-y))
    loss_vals[i] = loss
    i_vals[i] = i
    grad_y_pred = 2.0 * (y_pred-y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h*h*(1-h))

    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2

  training_percent_correct = round(100*(1-np.sum(abs(y_pred-y))/samples),1)
  print("Training set accuray: " + training_percent_correct.to_string(index=False))
  training_accuracy_vals[weight_num] = training_percent_correct.to_string(index=False)

  h = 1 / (1 + np.exp(-X_test.dot(w1)))
  y_pred = h.dot(w2)
  yt = y_test.values.reshape(-1, 1)
  testing_percent_correct = round(100*(1-np.sum(abs(y_pred-yt))/samples),1)
  print("Testing set accuracy: " + testing_percent_correct.to_string(index=False))
  testing_accuracy_vals[weight_num] = testing_percent_correct.to_string(index=False)

yh = y_pred - 0.5
yh = np.heaviside(yh, 1)

all_pos = np.sum(yh)
all_neg = samples - all_pos

fp = np.sum(np.multiply((yh-yt)>0,1))
fn = np.sum(np.multiply((yh-yt)<0,1))

tp = all_pos-fp
tn = all_neg-fn

precision = tp/(tp + fp)
recall = tp/(tp+fn)
f1 = 2.0*(precision*recall)/(precision + recall)

neg_precision = tn/(tn+fn)
neg_recall = tn/(tn+fp)
f2 = 2.0*(neg_precision*neg_recall)/(neg_precision + neg_recall)

accuracy = (tp + tn)/(tp+tn+fp+fn)

print("\nAccuracy: " + accuracy.to_string(index=False))

print("\nPrecision: " + precision.to_string(index=False))
print("Recall: " + recall.to_string(index=False))
print("F1 Score: " + f1.to_string(index=False))

print("\nNegative Precision: " + neg_precision.to_string(index=False))
print("Negative Recall: " + neg_recall.to_string(index=False))
print("Other F1 Score: " + f2.to_string(index=False))

plt.semilogy(i_vals,loss_vals,'-o')
plt.xlabel("Number of iterations")
plt.ylabel("Network Loss")
plt.title("Plot of Network Loss vs. Number of iterations")
plt.show()

plt.semilogy(weight_num_vals,training_accuracy_vals,'-og',weight_num_vals,testing_accuracy_vals,'-ob')
plt.xlabel("Number of Weights in network")
plt.ylabel("Network Accuracy")
plt.title("Plot of Network Accuracy vs. Number of Weights in Network")
plt.legend('Training accuracy','Testing accuracy')
plt.show()

"""**KNN MODEL**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

#Read in datafile
df= pd.read_csv("heart.csv")

df2 = pd.get_dummies(df, columns = ['Sex', 'ChestPainType', 'RestingECG', 'ST_Slope','ExerciseAngina'])
label = df2.pop('HeartDisease')
df2.insert( 20, 'HeartDisease', label)

##add code to show there are no Null values
#test train splitting
X = df2.iloc[:, :-1]
y = df2.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=55, test_size=0.3, shuffle=True)

import matplotlib.pyplot as plt
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

def minkowski_dist (a,b, p = 1):
    dimensions = len(a)
    distance = 0
    for d in range(dimensions):
        distance += abs(a[d] - b[d]**p)

    distance = distance**(1/p)
    return distance

def knn_predict(X_train, X_test, y_train, y_test, k, p):

    # Counter to help with label voting
    from collections import Counter

    # Make predictions on the test data
    # Need output of 1 prediction per test data point
    y_hat_test = []

    for test_point in X_test:
        distances = []

        for train_point in X_train:
            distance = minkowski_dist(test_point, train_point, p=p)
            distances.append(distance)

        # Store distances in a dataframe
        df_dists = pd.DataFrame(data=distances, columns=['dist'],
                                index=y_train.index)

        # Sort distances, and only consider the k closest points
        df_nn = df_dists.sort_values(by=['dist'], axis=0)[:k]

        # Create counter object to track the labels of k closest neighbors
        counter = Counter(y_train[df_nn.index])

        # Get most common label of all the nearest neighbors
        prediction = counter.most_common()[0][0]

        # Append prediction to output list
        y_hat_test.append(prediction)

    return y_hat_test


# Make predictions on test dataset
y_hat_test = knn_predict(X_train, X_test, y_train, y_test, k=6, p=1)

print(y_hat_test)
print(accuracy_score(y_test, y_hat_test))

accuracies = []
ks = range(1, 30)
for k in ks:
    knn = knn_predict(X_train, X_test, y_train, y_test, k=k, p=1)
    knn_accuracy = accuracy_score(y_test, knn)
    #print(knn_accuracy)
    accuracies.append(knn_accuracy)

fig, ax = plt.subplots()
ax.plot(ks, accuracies)
ax.set(xlabel="k",
       ylabel="Accuracy",
       title="Performance of knn")
plt.show()

